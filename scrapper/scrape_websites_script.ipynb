{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alika\\OneDrive\\Academic (Cloud)\\FAST\\Semester 8\\Data Science\\Ass 2\\pc-price-bot\\scrapper\\utils\\urls.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module sets up the necessary imports and configurations for web scraping using Selenium.\n",
    "It includes functions for configuring the Edge WebDriver, setting up logging, and utility functions\n",
    "for processing product data.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import(\n",
    "     TimeoutException,\n",
    "     StaleElementReferenceException,\n",
    "      NoSuchElementException\n",
    ")\n",
    "from typing import List, Tuple\n",
    "from time import sleep\n",
    "from drivers_setup.edge_driver_setup import configure_edge_webdriver\n",
    "from drivers_setup.logging_setup import (\n",
    "    setup_logging,\n",
    "    LogLevel\n",
    ")\n",
    "from utils.utility_functions import UtilityFunctions\n",
    "from utils.urls import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell initializes the UtilityFunctions class instance.\n",
    "\"\"\"\n",
    "utils = UtilityFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the\n",
    "DadduChargerScraper class which is used to scrape product\n",
    "data from the DadduCharger website.\n",
    "\"\"\"\n",
    "class DadduChargerScraper:\n",
    "    def __init__(self, driver_path: str, log_level=LogLevel.INFO):\n",
    "        self.driver = configure_edge_webdriver(driver_path)\n",
    "        self.logger = setup_logging(log_level)\n",
    "\n",
    "    def get_total_pages(self) -> int:\n",
    "        \"\"\"Extract the total number of pages from the pagination element.\"\"\"\n",
    "        try:\n",
    "            pagination = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'page-numbers'))\n",
    "            )\n",
    "            page_numbers = pagination.find_elements(By.CLASS_NAME, 'page-numbers')\n",
    "\n",
    "            last_page = max(\n",
    "                int(page.text) for page in page_numbers if page.text.isdigit()\n",
    "            )\n",
    "\n",
    "            return last_page\n",
    "\n",
    "        except TimeoutException:\n",
    "            self.logger.error(\"Pagination not found.\")\n",
    "            return 1\n",
    "\n",
    "    def scrape_category(self, url: str) -> List[Tuple[str, str, str, str, str, str, str]]:\n",
    "        \"\"\"Scrapes all pages of a category by dynamically counting total pages.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        total_pages = self.get_total_pages()\n",
    "        wait = WebDriverWait(self.driver, 10)\n",
    "        products = []\n",
    "\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            paginated_url = f\"{url}/?product-page={page_num}\"\n",
    "            self.driver.get(paginated_url)\n",
    "\n",
    "            try:\n",
    "                product_list = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'products')))\n",
    "                product_items = product_list.find_elements(By.CLASS_NAME, 'product')\n",
    "                for item in product_items:\n",
    "                    try:\n",
    "                        title_element = item.find_element(By.CLASS_NAME, 'woocommerce-loop-product__title')\n",
    "                        title = title_element.text\n",
    "\n",
    "                        price_element = item.find_elements(By.CLASS_NAME, 'price')\n",
    "                        price_text = price_element[0].text if price_element else 'N/A'\n",
    "                        price = utils.extract_price(price_text)\n",
    "\n",
    "                        product_url = item.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                        image_url = item.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "\n",
    "                        vendor = title.split()[0] if title else \"Unknown\"\n",
    "                        condition = \"Used\" if \"Used\" in title else \"New\"\n",
    "                        category = utils.categorize_product(title)\n",
    "\n",
    "                        #availability = self.scrape_availability(product_url)\n",
    "\n",
    "                        products.append((utils.clean_title(title), price, category, vendor, condition, \"DadduCharger\", product_url, image_url))\n",
    "\n",
    "                    except StaleElementReferenceException:\n",
    "                        self.logger.error(\"Stale element reference error while scraping a product.\")\n",
    "\n",
    "            except TimeoutException:\n",
    "                self.logger.error(f\"Timeout while loading page {page_num} for URL: {url}\")\n",
    "                break\n",
    "\n",
    "        return products\n",
    "\n",
    "\n",
    "    def scrape_all_categories(self) -> List[Tuple[str, str, str, str, str, str, str]]:\n",
    "        \"\"\"Scrapes all predefined category URLs.\"\"\"\n",
    "        all_products = []\n",
    "        for url in dadducharger_urls:\n",
    "            self.logger.info(f\"Scraping category: {url}\")\n",
    "            products = self.scrape_category(url)\n",
    "            all_products.extend(products)\n",
    "        return all_products\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraping process.\"\"\"\n",
    "        try:\n",
    "            data = self.scrape_all_categories()\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "        utils.save_to_csv(data, folder=\"output_data\", filename=\"dadducharger_products.csv\")\n",
    "        self.logger.info(\"Data saved to dadducharger_products.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = DadduChargerScraper(WEB_DRIVER_EXECUTABLE_PATH)\n",
    "    scraper.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TechMatchedScraper class is used to scrape product data from the TechMatched website.\n",
    "\n",
    "Methods:\n",
    "    get_last_page_number: Extracts the last page number from the pagination element.\n",
    "    scrape_category: Scrapes all products from a given category URL.\n",
    "    scrape_all_categories: Scrapes all predefined category URLs.\n",
    "    save_to_csv: Saves scraped data to a CSV file.\n",
    "    run: Runs the scraping process.\n",
    "\"\"\"\n",
    "class TechMatchedScraper:\n",
    "    def __init__(self, driver_path: str, log_level=LogLevel.INFO):\n",
    "        self.driver = configure_edge_webdriver(driver_path)\n",
    "        self.wait = WebDriverWait(self.driver, 5)\n",
    "        self.logger = setup_logging(log_level)\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_category(self, url: str) -> List[Tuple[str, str, str, str, str, str, str]]:\n",
    "        products = []\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            full_url = f\"{url}page/{page}/\"\n",
    "            self.logger.info(f\"Scraping page {page} of {url}...\")\n",
    "            self.driver.get(full_url)\n",
    "\n",
    "            try:\n",
    "                product_list = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'products')))\n",
    "                product_items = product_list.find_elements(By.CLASS_NAME, 'product')\n",
    "\n",
    "                if not product_items:\n",
    "                    break\n",
    "\n",
    "                for item in product_items:\n",
    "                    title_element = item.find_element(By.CLASS_NAME, 'woocommerce-loop-product__title')\n",
    "                    title = title_element.text\n",
    "\n",
    "                    price_element = item.find_elements(By.CLASS_NAME, 'price')\n",
    "                    price_text = price_element[0].text if price_element else 'N/A'\n",
    "                    price = utils.extract_price(price_text)\n",
    "\n",
    "                    product_url = item.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    image_url = item.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "\n",
    "                    vendor = title.split()[0] if title else \"Unknown\"\n",
    "                    condition = \"Used\" if \"Used\" in title else \"New\"\n",
    "\n",
    "                    category = utils.categorize_product(title)\n",
    "\n",
    "                    website = \"TechmMatched\"\n",
    "                    products.append((utils.clean_title(title), price, category, vendor, condition,website, product_url, image_url))\n",
    "\n",
    "                last_page = self.get_last_page_number()\n",
    "                if page >= last_page:\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "\n",
    "            except TimeoutException:\n",
    "                self.logger.error(f\"Timeout while loading page {page}.\")\n",
    "                break\n",
    "            except StaleElementReferenceException:\n",
    "                self.logger.error(\"Stale element reference error.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error: {e}\")\n",
    "                break\n",
    "\n",
    "        return products\n",
    "\n",
    "    def scrape_all_categories(self) -> List[Tuple[str, str, str, str, str, str, str]]:\n",
    "        all_products = []\n",
    "        for url in techmatched_urls:\n",
    "            all_products.extend(self.scrape_category(url))\n",
    "        return all_products\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraping process.\"\"\"\n",
    "        try:\n",
    "            data = self.scrape_all_categories()\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "        utils.save_to_csv(data,folder=\"output_data\", filename=\"techmatched_products.csv\")\n",
    "        self.logger.info(\"Data saved to techmatched_products.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = configure_edge_webdriver(WEB_DRIVER_EXECUTABLE_PATH)\n",
    "    scraper = TechMatchedScraper(WEB_DRIVER_EXECUTABLE_PATH)\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JunaidTechScraper class is used to scrape product data from the JunaidTech website.\n",
    "\n",
    "Methods:\n",
    "    get_total_pages: Extracts the total number of pages from the pagination element.\n",
    "    scrape_category: Scrapes all products from a given category URL.\n",
    "    scrape_all_categories: Scrapes all predefined category URLs.\n",
    "    run: Runs the scraping process.\n",
    "\"\"\"\n",
    "class JunaidTechScraper:\n",
    "    def __init__(self, driver_path: str, log_level=LogLevel.INFO):\n",
    "        self.driver = configure_edge_webdriver(driver_path)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.logger = setup_logging(log_level)\n",
    "\n",
    "    def get_total_pages(self) -> int:\n",
    "        \"\"\"Extracts the total number of pages from the pagination element.\"\"\"\n",
    "        try:\n",
    "            pagination = self.wait.until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'pagination'))\n",
    "            )\n",
    "            page_numbers = pagination.find_elements(By.CLASS_NAME, 'PageNumber')\n",
    "            last_page = max(int(page.text) for page in page_numbers if page.text.isdigit())\n",
    "            return last_page\n",
    "        except TimeoutException:\n",
    "            self.logger.error(\"Pagination not found.\")\n",
    "            return 1\n",
    "\n",
    "    def scrape_category(self, url: str) -> List[Tuple[str, str, str, str, str, str, str, str]]:\n",
    "        \"\"\"Scrapes all pages of a category by dynamically counting total pages.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        total_pages = self.get_total_pages()\n",
    "        products = []\n",
    "\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            paginated_url = f\"{url}?sort=2&page={page_num}\"\n",
    "            print(paginated_url)\n",
    "            self.driver.get(paginated_url)\n",
    "            try:\n",
    "                self.wait.until(EC.presence_of_element_located((By.ID, 'divListView')))\n",
    "\n",
    "                product_items = self.driver.find_elements(By.XPATH, \"//div[contains(@class, 'item') and contains(@class, 'list-view')]\")\n",
    "\n",
    "                for item in product_items:\n",
    "                    try:\n",
    "                        title_element = item.find_element(By.XPATH, \".//h4[@name='list-productname']/a\")\n",
    "                        title = title_element.text.strip()\n",
    "                        product_url = paginated_url + title_element.get_attribute(\"href\")\n",
    "\n",
    "                        image_element = item.find_element(By.XPATH, \".//div[@class='image']/a/img\")\n",
    "                        image_url = image_element.get_attribute(\"src\")\n",
    "\n",
    "                        try:\n",
    "                            price_element = item.find_element(By.CLASS_NAME, 'price')\n",
    "                            price = price_element.text.strip()\n",
    "                        except NoSuchElementException:\n",
    "                            price = \"N/A\"\n",
    "\n",
    "                        vendor = title.split()[0] if title else \"Unknown\"\n",
    "                        condition = \"Used\" if \"Used\" in title else \"New\"\n",
    "\n",
    "                        category = UtilityFunctions.categorize_product(title)\n",
    "                        website = \"JunaidTech\"\n",
    "\n",
    "                        products.append((UtilityFunctions.clean_title(title), price, category, vendor, condition, website, product_url, image_url))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting product details: {e}\")\n",
    "            except TimeoutException:\n",
    "                self.logger.error(f\"Timeout while loading page {page_num} for URL: {url}\")\n",
    "                break\n",
    "\n",
    "        return products\n",
    "\n",
    "    def scrape_all_categories(self) -> List[Tuple[str, str, str, str, str, str, str, str]]:\n",
    "        all_products = []\n",
    "        for url in junaidtech_urls:\n",
    "            all_products.extend(self.scrape_category(url))\n",
    "        return all_products\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraping process.\"\"\"\n",
    "        try:\n",
    "            data = self.scrape_all_categories()\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "        utils.save_to_csv(data, folder=\"output_data\", filename=\"junaidtech_products.csv\")\n",
    "        self.logger.info(\"Data saved to junaidtech_products.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = JunaidTechScraper(WEB_DRIVER_EXECUTABLE_PATH)\n",
    "    scraper.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Page Items: [\"You're currently reading page\\n1\", 'Page\\n2', 'Page\\n3', 'Page\\n4', 'Page\\n5', 'Page\\nNext']\n",
      "Extracted Page Numbers: []\n",
      "Total pages for https://www.paklap.pk/laptops-prices.html: 1\n",
      "Scraping: https://www.paklap.pk/laptops-prices.html?p=1\n",
      "Pagination not found. Assuming 1 page.\n",
      "Total pages for https://www.paklap.pk/accessories.html: 1\n",
      "Scraping: https://www.paklap.pk/accessories.html?p=1\n",
      "Pagination not found. Assuming 1 page.\n",
      "Total pages for https://www.paklap.pk/used-laptop-price-pakistan.html: 1\n",
      "Scraping: https://www.paklap.pk/used-laptop-price-pakistan.html?p=1\n",
      "Extracted Page Items: [\"You're currently reading page\\n1\", 'Page\\n2', 'Page\\n3', 'Page\\nNext']\n",
      "Extracted Page Numbers: []\n",
      "Total pages for https://www.paklap.pk/apple-products.html: 1\n",
      "Scraping: https://www.paklap.pk/apple-products.html?p=1\n",
      "Data saved to output_data\\paklap_products.csv\n",
      "Data saved to paklap_products.csv\n"
     ]
    }
   ],
   "source": [
    "class PakLapScraper:\n",
    "    def __init__(self, driver_path: str, log_level=LogLevel.INFO):\n",
    "        self.driver = configure_edge_webdriver(driver_path)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.logger = setup_logging(log_level)\n",
    "\n",
    "    import re\n",
    "\n",
    "    def get_total_pages(self) -> int:\n",
    "        \"\"\"Extracts the total number of pages by counting numeric page links.\"\"\"\n",
    "        try:\n",
    "            pagination = self.wait.until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'pages'))\n",
    "            )\n",
    "            page_items = pagination.find_elements(By.CSS_SELECTOR, '.pages-items li')\n",
    "\n",
    "            page_texts = [item.text.strip() for item in page_items]\n",
    "            print(f\"Extracted Page Items: {page_texts}\")\n",
    "\n",
    "            page_numbers = [int(num) for text in page_texts for num in re.findall(r'\\d+', text)]\n",
    "\n",
    "            print(f\"Extracted Page Numbers: {page_numbers}\")\n",
    "\n",
    "            return max(page_numbers) if page_numbers else 1\n",
    "        except TimeoutException:\n",
    "            print(\"Pagination not found. Assuming 1 page.\")\n",
    "            return 1\n",
    "\n",
    "    def scrape_category(self, url: str) -> List[Tuple[str, str, str, str, str, str, str]]:\n",
    "        \"\"\"Scrapes all pages of a category dynamically.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        total_pages = self.get_total_pages()\n",
    "        print(f\"Total pages for {url}: {total_pages}\")\n",
    "\n",
    "        products = []\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            paginated_url = f\"{url}&p={page_num}\" if \"?\" in url else f\"{url}?p={page_num}\"\n",
    "            print(f\"Scraping: {paginated_url}\")\n",
    "            self.driver.get(paginated_url)\n",
    "\n",
    "            try:\n",
    "                product_list = self.wait.until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, 'products'))\n",
    "                )\n",
    "                product_items = product_list.find_elements(By.CLASS_NAME, 'product-item')\n",
    "\n",
    "                for item in product_items:\n",
    "                    try:\n",
    "                        title_element = item.find_element(By.CLASS_NAME, 'product-item-link')\n",
    "                        title = title_element.text.strip()\n",
    "\n",
    "                        price_element = item.find_elements(By.CLASS_NAME, 'price')\n",
    "                        price_text = price_element[0].text if price_element else 'N/A'\n",
    "                        price = utils.extract_price(price_text)\n",
    "\n",
    "                        product_url = title_element.get_attribute('href')\n",
    "                        image_url = item.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "\n",
    "                        vendor = title.split()[0] if title else \"Unknown\"\n",
    "                        condition = \"Used\" if \"Used\" in title else \"New\"\n",
    "                        category = utils.categorize_product(title)\n",
    "\n",
    "                        products.append((utils.clean_title(title), price, category, vendor, condition, \"PakLap\", product_url, image_url))\n",
    "\n",
    "                    except StaleElementReferenceException:\n",
    "                        print(\"Stale element reference error while scraping a product.\")\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout while loading page {page_num} for URL: {url}\")\n",
    "                break\n",
    "\n",
    "        return products\n",
    "\n",
    "\n",
    "    def scrape_all_categories(self):\n",
    "        all_products = []\n",
    "        for url in paklap_urls:\n",
    "            all_products.extend(self.scrape_category(url))\n",
    "        return all_products\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraping process.\"\"\"\n",
    "        try:\n",
    "            data = self.scrape_all_categories()\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "        utils.save_to_csv(data, folder=\"output_data\", filename=\"paklap_products.csv\")\n",
    "        print(\"Data saved to paklap_products.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PakLapScraper(WEB_DRIVER_EXECUTABLE_PATH)\n",
    "    scraper.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
